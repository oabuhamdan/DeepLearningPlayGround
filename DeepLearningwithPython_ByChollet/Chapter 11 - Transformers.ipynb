{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.4 The Transformer architecture\n",
    "Transformers were introduced in the seminal paper “Attention is all you need”. “Neural Attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers.\n",
    "### Understanding self-attention\n",
    "Not all input information seen by a model is equally important to the task at hand, so models should “pay more attention” to some features and “pay less attention” to other features.\n",
    "Does that sound familiar? You’ve already encountered a similar concept twice in this book:\n",
    "- Max pooling in convnets looks at a pool of features in a spatial region and selects just one feature to keep. That’s an “all or nothing” form of attention: keep the most important feature and discard the rest.\n",
    "- TF-IDF normalization assigns importance scores to tokens based on how much information different tokens are likely to carry. Important tokens get boosted while irrelevant tokens get faded out. That’s a continuous form of attention.\n",
    "\n",
    "\n",
    "Word embeddings—vector spaces capture the “shape” of the semantic relationships between different words. In an embedding space, a single word has a fixed position, i.e. a fixed set of relationships with every other word in the space. But that’s not quite how language works, the meaning of a word is usually context-specific.\n",
    "\n",
    "Clearly, a smart embedding space would provide a different vector representation for a word depending on the other words surrounding it. That’s where **self-attention** comes in. The purpose of **self-attention** is to modulate the representation of a token by using the representations of related tokens in the sequence. \n",
    "\n",
    "![alt text](static/self-attention.png \"self-attention\")\n",
    "\n",
    "For every word in the sentence, we'll do the following:\n",
    "1. Compute relevancy scores between the vector for the word and every other word in the sentence. These are the attention scores. It's the dot product between two word vectors as a measure of the strength of their relationship.\n",
    "2. Compute the sum of all word vectors in the sentence, weighted by our relevancy scores. Words closely related to the word will contribute more to the sum (including the word itself), while irrelevant words will contribute almost nothing.\n",
    "\n",
    "A Transformer is a sequence-to-sequence model: it was designed to convert one sequence into another.\n",
    "\n",
    "![alt text](static/simple-self-attention.png \"self-attention\")\n",
    "This means “for each token in inputs (A), compute how much the token is related to every token in inputs (B), and use these scores to weight a sum of tokens from inputs (C).”\n",
    "\n",
    "In the general case, you could be doing this with three different sequences. We’ll call them “query,” “keys,” and “values.” The operation becomes “for each element in the query, compute how much the element is related to every key, and use these scores to weight a sum of values”\n",
    "```\n",
    "outputs = sum(values * pairwise_score(query, keys)\n",
    "```\n",
    "\n",
    "So, if you’re just doing sequence classification, then query, keys, and values are all the same: you’re comparing a sequence to itself, to enrich each token with context from the whole sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f194039afb70b97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-head attention\n",
    "Multi-head attention is an improvement to self-attention from the “Attention is All You Need” paper. It works by dividing the attention process into several smaller parts called heads, which each learn differently. We create three versions of the input (query, key, and value) using different small neural networks, resulting in three different sets of vectors. Each set of vectors is processed separately through attention, and then all the results are combined into one final output.\n",
    "\n",
    "![alt text](static/multi-head-attention.png \"mh-attention\")\n",
    "\n",
    "Having independent heads helps the layer learn different groups of features for each token, where features within one group are correlated with each other but are mostly independent of features in a different group.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d14c103c541dc7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Transformer Architecture\n",
    "If adding extra dense projections is so useful, why don’t we also apply one or two to the output of the attention mechanism? Actually, that’s a great idea—let’s do that.\n",
    "1. Let's add residual connections to make sure we don’t destroy any valuable information along the way.\n",
    "2. Let's add normalization layers, which are supposed to help gradients flow better during backpropagation.\n",
    "\n",
    "The first Dense Layer in the Dense projection: It is used in the first Dense layer of the dense_proj sequential model to increase the dimensionality of the input vectors temporarily.\n",
    " \n",
    "The second Dense layer in the Dense Projection: To ensure that the output of the dense projection matches the input dimension for residual connections.\n",
    "\n",
    "![alt text](static/transformers.png \"mh-attention\")\n",
    "Figure: The TransformerEncoder chains a MultiHeadAttention layer with a dense projection and adds normalization as well as residual connections.\n",
    "\n",
    "The original Transformer architecture consists of two parts:\n",
    "1. a Transformer encoder that processes the source sequence.\n",
    "2. Transformer decoder that uses the source sequence to generate a translated version.\n",
    "\n",
    "The encoder part can be used for text classification. It’s a very generic module that ingests a sequence and learns to turn it into a more useful representation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bd51b9ffed6aa0d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T23:51:46.994929Z",
     "start_time": "2024-07-28T23:51:45.126581Z"
    }
   },
   "id": "d5338bf075621e59"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"data/aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"data/aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"data/aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_vectorization = layers.TextVectorization(output_mode=\"int\")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T20:59:12.766381Z",
     "start_time": "2024-07-28T20:59:01.595943Z"
    }
   },
   "id": "69b000f501424d47"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# implementing the previous architecture\n",
    "# trying the transformer encoder on the movie review sentiment classification task.\n",
    "\n",
    "# embed dim: It is used as the key_dim for the MultiHeadAttention layer, which determines the dimension of the query, key, and value vectors in the attention mechanism. It is also the output dimension of the dense_proj layer, ensuring that the output of the dense projection matches the input dimension for residual connections.\n",
    "\n",
    "# dense dim: It is used in the first Dense layer of the dense_proj sequential model to increase the dimensionality of the input vectors temporarily. The second Dense layer projects this higher-dimensional representation back to the embed_dim.\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim  # Size of the input token vectors\n",
    "        self.dense_dim = dense_dim  # Size of the inner dense layer\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        # BatchNormalization doesn't work well for sequence data. Instead, we’re using the LayerNormalization layer, which normalizes each sequence independently of other sequences in the batch\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):  # Serialization to save the model\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# When you write custom layers, make sure to implement the get_config method: this enables the layer to be reinstantiated from its config dict, which is useful during model saving and loading. The method should return a Python dict that contains the values of the constructor arguments used to create the layer."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:23:20.243424Z",
     "start_time": "2024-07-30T00:23:20.235803Z"
    }
   },
   "id": "d24edeb46c34c84c"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_12 (Tr  (None, None, 256)         543776    \n",
      " ansformerEncoder)                                               \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5664033 (21.61 MB)\n",
      "Trainable params: 5664033 (21.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T21:07:59.869343Z",
     "start_time": "2024-07-28T21:07:58.676875Z"
    }
   },
   "id": "ac564f7d6ab83307"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/625 [..............................] - ETA: 2:19:48 - loss: 1.6602 - accuracy: 0.4375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     keras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/transformer_encoder.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m                                     save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m ]\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mint_train_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mint_val_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# stopped this cell since it's very expensive and takes a lot of time\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1799\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1800\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1801\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1804\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1805\u001B[0m ):\n\u001B[1;32m   1806\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1807\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1809\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    829\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 832\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    835\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    865\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    867\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 868\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    872\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    874\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1319\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1321\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1322\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1323\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1325\u001B[0m     args,\n\u001B[1;32m   1326\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1327\u001B[0m     executing_eagerly)\n\u001B[1;32m   1328\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1484\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1486\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1487\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1488\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1490\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1491\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1492\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1493\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1494\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1495\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1496\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1500\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1501\u001B[0m   )\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# stopped this cell since it's very expensive and takes a lot of time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T21:08:17.164917Z",
     "start_time": "2024-07-28T21:08:00.519316Z"
    }
   },
   "id": "b28cf93c56c3d5d6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1078s 1s/step - loss: 0.3631 - accuracy: 0.8417\n",
      "Test acc: 0.842\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/transformer_encoder.keras\",\n",
    "                                custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T19:17:54.973824Z",
     "start_time": "2024-07-27T18:59:53.977171Z"
    }
   },
   "id": "342f21c5f9e5edbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Transformer encoder you just saw in action wasn't a sequence model at all.\n",
    "\n",
    "You could change the order of the tokens in a sequence, and you’d get the exact same pairwise attention scores and the exact same context-aware representations. If you were to completely scramble the words in every movie review, the model wouldn't notice, and you’d still get the exact same accuracy.\n",
    "\n",
    "Self-attention is a set-processing mechanism, focused on the relationships between pairs of sequence elements. It’s blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence.\n",
    "\n",
    "![alt text](static/nlp_compare.png \"nlp compare\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d926c9d3ab2fdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using positional encoding to re-inject order information\n",
    "Transformer is a hybrid approach that is technically order-agnostic, but that manually injects order information in the representations it processes.\n",
    "This is the missing ingredient! It’s called **positional encoding.**\n",
    "\n",
    "We're going to add the word's position in the sentence to each word embedding. \n",
    "Our input word embeddings will have two components:\n",
    "- A word vector: represents the word independently of any specific context.\n",
    "- A position vector: represents the position of the word in the current sentence.\n",
    "\n",
    "Neural networks don’t like very large input values, or discrete input distributions!\n",
    "Two solutions are used:\n",
    "1. Use the Cosine function to squash the positional values in the range [-1, 1] (used in the \"attention all you need\" paper)\n",
    "2. Learn position-embedding vectors the same way we learn to embed word indices. Then, proceed to add our position embeddings to the corresponding word embeddings (used in the book)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d6803506f1a3adf"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# A reminder: \n",
    "# The input dim is the size of the vocabulary, i.e., the total number of unique tokens (words) in the input data.\n",
    "# The output dim is the representation power of each word embedding.\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # original word embeddings\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        # new position embeddings\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # get the length of the input\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        # tensor of positions\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        # get the token embeddings\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        # get the position embeddings\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        # add both\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # this will be called automatically by the framework to mask zero padded input\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:23:25.477742Z",
     "start_time": "2024-07-30T00:23:25.469444Z"
    }
   },
   "id": "b4ee2b05c98d3e6f"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m inputs \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m,), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint64\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m x \u001B[38;5;241m=\u001B[39m PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n\u001B[0;32m---> 10\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mTransformerEncoder\u001B[49m(embed_dim, dense_dim, num_heads)(x)\n\u001B[1;32m     11\u001B[0m x \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mGlobalMaxPooling1D()(x)\n\u001B[1;32m     12\u001B[0m x \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mDropout(\u001B[38;5;241m0.5\u001B[39m)(x)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'TransformerEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"models/full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "# stopped it because it takes a lot of time."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:23:11.363880Z",
     "start_time": "2024-07-30T00:23:10.309900Z"
    }
   },
   "id": "71d4ede654623484"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## When to use Sequence Models over Bag of Words?\n",
    "You may sometimes hear that bag-of-words methods are outdated, and that Transformer-based sequence models are the way to go, no matter what task or dataset you’re looking at. This is definitely not the case: a small stack of Dense layers on top of a bag-of-bigrams remains a perfectly valid and relevant approach in many cases.\n",
    "\n",
    "In 2017, my team and I (the author) ran a systematic analysis of the performance of various text-classification techniques across many different types of text datasets, and we discovered a remarkable and surprising rule of thumb for deciding whether to go with a bag-of-words model or a sequence model.\n",
    "\n",
    "It turns out that when approaching a new text-classification task, you should pay close attention to **the ratio between the number of samples in your training data and the mean number of words per sample**.:\n",
    "- If that ratio is small -less than 1,500- then use the bag-of-bigrams model will perform better\n",
    "- If that ratio is higher than 1,500, then you should go with a sequence model.\n",
    "\n",
    "In other words, **sequence models work best when lots of training data is available and when each sample is relatively short**.\n",
    "\n",
    "![alt text](static/bow-vs-sequence.png \"bag of words vs sequence model\")\n",
    "\n",
    "Now, keep in mind that this heuristic rule was developed specifically for text classification. It may not necessarily hold for other NLP tasks—when it comes to machine translation. For instance, Transformer shines especially for very long sequences, compared to RNNs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12763050fe3f8358"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beyond text classification: Sequence-to-sequence learning\n",
    "A sequence-to-sequence model takes a sequence as input (often a sentence or paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
    "- Machine translation: Convert a paragraph in a source language to its equivalent in a target language.\n",
    "- Text summarization: Convert a long document to a shorter version that retains the most important information.\n",
    "- Question answering: Convert an input question into its answer.\n",
    "- Chatbots: Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
    "- Text generation: Convert a text prompt into a paragraph that completes the prompt.\n",
    "\n",
    "The general template behind sequence-to-sequence models is described in the next figure. During training:\n",
    "- An encoder model turns the source sequence into an intermediate representation.\n",
    "- A decoder is trained to predict the next token i in the target sequence by looking at both previous tokens (0 to i - 1) and the encoded source sequence.\n",
    "\n",
    "![alt text](static/seq2seq.png \"sequence to sequence\")\n",
    "\n",
    "During inference, we don’t have access to the target sequence. We’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
    "- We obtain the encoded source sequence from the encoder.\n",
    "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\"), and uses them to predict the first real token in the sequence.\n",
    "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string \"[end]\").\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "827dfebf1607f156"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-28 17:14:19--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4023:1009::cf, 2607:f8b0:4023:1004::cf, 2607:f8b0:4023:1002::cf, ...\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4023:1009::cf|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2638744 (2.5M) [application/zip]\r\n",
      "Saving to: ‘./data/spa-eng.zip’\r\n",
      "\r\n",
      "spa-eng.zip         100%[===================>]   2.52M  7.48MB/s    in 0.3s    \r\n",
      "\r\n",
      "2024-07-28 17:14:19 (7.48 MB/s) - ‘./data/spa-eng.zip’ saved [2638744/2638744]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# We'll do machine translation.\n",
    "# We'll be working with an English-to-Spanish translation dataset available at www.manythings.org/anki/. \n",
    "# It's a dataset of tab separated words.\n",
    "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip -P ./data\n",
    "!unzip -q ./data/spa-eng.zip -d ./data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T22:14:20.238449Z",
     "start_time": "2024-07-28T22:14:18.603148Z"
    }
   },
   "id": "8bc2ceaa6affc410"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', '[start] Ve. [end]'), ('Go.', '[start] Vete. [end]'), ('Go.', '[start] Vaya. [end]'), ('Go.', '[start] Váyase. [end]'), ('Hi.', '[start] Hola. [end]'), ('Run!', '[start] ¡Corre! [end]'), ('Run.', '[start] Corred. [end]'), ('Who?', '[start] ¿Quién? [end]'), ('Fire!', '[start] ¡Fuego! [end]'), ('Fire!', '[start] ¡Incendio! [end]')]\n",
      "[(\"You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\", \"[start] No puedes ver contenido en Flash en un iPad. Sin embargo, puedes fácilmente enviarte por correo electrónico las URL's de esas páginas web y ver el contenido en tu computadora cuando llegas a casa. [end]\"), ('A mistake young people often make is to start learning too many languages at the same time, as they underestimate the difficulties and overestimate their own ability to learn them.', '[start] Un error que cometen a menudo los jóvenes es el de comenzar a aprender demasiadas lenguas al mismo tiempo, porque subestiman sus dificultades y sobrestiman sus propias capacidades para aprenderlas. [end]'), (\"No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\", '[start] No importa cuánto insistas en convencer a la gente de que el chocolate es vainilla, seguirá siendo chocolate, aunque puede que te convenzas a ti mismo y a algunos otros de que es vainilla. [end]'), ('In 1969, Roger Miller recorded a song called \"You Don\\'t Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It\\'s the first song he wrote and sang that became popular.', '[start] En 1969, Roger Miller grabó una canción llamada \"Tú no quieres mi amor\". Hoy, esta canción es más conocida como \"En el verano\". Es la primera canción que escribió y cantó que se convirtió popular. [end]'), ('A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.', '[start] Un niño que es hablante nativo normalmente sabe muchas cosas acerca de su lengua que un hablante no nativo que lo haya estado estudiando durante muchos años no sabe todavía y que quizá no sabrá nunca. [end]'), ('There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.', '[start] Hay cuatro causas principales de muertes relacionadas con el alcohol. Lesión por un accidente automovilístico o violencia es una. Enfermedades como cirrosis del hígado, cáncer, enfermedades del corazón y del sistema circulatorio son las otras. [end]'), (\"There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\", '[start] Hay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente dinero para la educación universitaria de sus hijos. [end]'), ('A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.', '[start] Una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque están preocupados acerca del cambio climático. [end]'), ('Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.', '[start] Como suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a la siguiente página encontrada por Google y espero encontrar algo menos irritante. [end]'), ('If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.', '[start] Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado. [end]')]\n"
     ]
    }
   ],
   "source": [
    "text_file = \"data/spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))\n",
    "# the file contains words and sentences\n",
    "print(text_pairs[:10])\n",
    "print(text_pairs[-10:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T22:16:31.912964Z",
     "start_time": "2024-07-28T22:16:31.813001Z"
    }
   },
   "id": "55ada5936c807141"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# shuffle the data and split it\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T22:17:57.916875Z",
     "start_time": "2024-07-28T22:17:57.913950Z"
    }
   },
   "id": "bffde2b31c6ae05b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let’s prepare two separate TextVectorization layers: one for English and one for Spanish. We’re going to need to customize the way strings are preprocessed:\n",
    "- We need to preserve the \"[start]\" and \"[end]\" tokens that we’ve inserted. By default, the characters \\[ and \\] would be stripped, but we want to keep them around so we can tell apart the word “start” and the start token \"[start]\".\n",
    "- Punctuation is different from language to language! In the Spanish TextVectorization layer, if we’re going to strip punctuation characters, we need to also strip the character ¿.\n",
    "\n",
    "Note that for a non-toy translation model, we would treat punctuation characters as separate tokens rather than stripping them, since we would want to be able to generate correctly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeee8f5594ca63ed"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:51:52.896095: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-07-28 18:51:52.896182: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-28 18:51:52.896351: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-28 18:51:52.896582: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-28 18:51:52.896908: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-07-28 18:51:54.069373: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"  # add the flipped ? to the chars to be stripped\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")  # remove [ from the chars to be stripped\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")  # remove ] from the chars to be stripped\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "\n",
    "# We’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "# English Vectorization\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Spanish Vectorization\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    # Why one extra token? The target sequence fed to the decoder during training is the ground truth sentence shifted by one position. This helps the model learn to predict the next token in the sequence. \n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]  # pair 0 is the english word\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]  # pair 1 is the spanish word\n",
    "source_vectorization.adapt(train_english_texts)  # adapt the english text\n",
    "target_vectorization.adapt(train_spanish_texts)  # adapt the spanish text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T23:51:58.509037Z",
     "start_time": "2024-07-28T23:51:52.826380Z"
    }
   },
   "id": "ceb721472b3da4e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Finally, we can turn our data into a tf.data pipeline. We want it to return a tuple (inputs, target) where inputs is a dict with two keys, “encoder_inputs” (the English sentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanish sentence offset by one step ahead.\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    # The input Spanish sentence doesn't include the last token to keep inputs and targets at the same length.\n",
    "    # The target Spanish sentence is one step ahead. Both are still the same length (20 words).\n",
    "    return ({\n",
    "                \"english\": eng,\n",
    "                \"spanish\": spa[:, :-1],\n",
    "            }, spa[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    # prefetch(16): This means that the data pipeline will keep 16 batches in the buffer ready to be used. \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T23:52:03.139055Z",
     "start_time": "2024-07-28T23:52:02.370398Z"
    }
   },
   "id": "3cf70bd602908614"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:59:31.058922: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "# the output shows that each item of the inputs is in size of 64 (batch size) and length of 20 (sequence length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T23:59:31.066532Z",
     "start_time": "2024-07-28T23:59:30.880449Z"
    }
   },
   "id": "b8ff820edcef77f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is now Ready.\n",
    "Let's build some Models.\n",
    "## Sequence 2 Sequence Learning with RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "314762661a597605"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length,))\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(16, return_sequences=True)(x)\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c30f8ebf6e3db20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two flaws with this approach:\n",
    "1. The target sequence must always be the same length as the source sequence. In practice, this is rarely the case. Technically, this isn’t critical, as you could always pad either the source sequence or the target sequence to make their lengths match.\n",
    "2. Due to the step-by-step nature of RNNs, the model will only be looking at tokens 0...N in the source sequence in order to predict token N in the target sequence. This constraint makes this setup unsuitable for most tasks, and particularly translation. Consider translating “The weather is nice today” to French—that would be “Il fait beau aujourd’hui.” You’d need to be able to predict “Il” from just “The,” “Il fait” from just “The weather,” etc., which is simply impossible.\n",
    "\n",
    "If you’re a human translator, you’d start by reading the entire source sentence before starting to translate it. This is especially important if you’re dealing with languages that have wildly different word ordering, like English and Japanese. And that’s exactly what standard sequence-to-sequence models do."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c224c634082267"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a proper sequence-to-sequence setup, you would first use an RNN **(the encoder)** to turn the source sequence into a single vector (or set of vectors). This could be the last output of the RNN, or alternatively, its final internal state vectors. Then you would use this vector (or vectors) as the initial state of another RNN **(the decoder)**, which would look at elements 0...N in the target sequence, and try to predict step N+1 in the target sequence\n",
    "\n",
    "![alt text](static/seq2seq_model.png \"sequence2seq model\")\n",
    "\n",
    "This figure explains things in more details\n",
    "\n",
    "<img src=\"static/seq2seq_detailed.png\" width=\"800\">\n",
    "\n",
    "Notice that in the RNN-based sequence-to-sequence (Seq2Seq) models, the input sequence must be fully processed by the encoder before it can be sent to the decoder.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a12c3c427cef1b5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Defining the Encoder \n",
    "embed_dim = 256\n",
    "# The latent_dim parameter in the Bidirectional GRU layer defines the size of the output space of the GRU layer, which is essentially the dimensionality of the intermediate representation produced by the GRU units. This output will be fed to the decoder.\n",
    "latent_dim = 1024\n",
    "\n",
    "english_input = keras.Input(shape=(sequence_length,), name='english')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(english_input)\n",
    "# Merge Mode is the Bidirectional layer specifies how to combine the outputs from the forward and backward GRU layers.\n",
    "# Here, the outputs are summed. Other options for merge_mode include \"concat\" (concatenates outputs from both directions) and \"ave\" (averages the outputs).\n",
    "encoded_english = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T21:30:58.176176Z",
     "start_time": "2024-07-29T21:30:56.779834Z"
    }
   },
   "id": "63d7b19d91cb9d55"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Defining the Decoder \n",
    "# In training, we use the true previous tokens to predict the next token in the sequence. This is called \"teacher forcing.\"\n",
    "# past_target is the true sequence of tokens from the Spanish sentence.\n",
    "spanish_input = keras.Input(shape=(sequence_length,), name='spanish')\n",
    "# The embedding layer converts each token into a dense vector representation.\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(spanish_input)\n",
    "\n",
    "# The GRU processes the entire sequence at once, using the encoded_source as the initial state.\n",
    "# The GRU layer outputs a sequence of hidden states, one for each time step.\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_english)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "spanish_output = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "seq2seq_rnn = keras.Model([english_input, spanish_input], spanish_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T21:30:59.654537Z",
     "start_time": "2024-07-29T21:30:58.180963Z"
    }
   },
   "id": "8d5a14864585943c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 121/1302 [=>............................] - ETA: 9:39 - loss: 5.5182 - accuracy: 0.2530"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m seq2seq_rnn\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m      2\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrmsprop\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m     loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m     metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 5\u001B[0m \u001B[43mseq2seq_rnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_ds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py:1813\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1811\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[1;32m   1812\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[0;32m-> 1813\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[1;32m   1815\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:475\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[1;32m    469\u001B[0m \n\u001B[1;32m    470\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[0;32m--> 475\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:322\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[0;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 322\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    325\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    327\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:345\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[0;34m(self, mode, batch, logs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[0;32m--> 345\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[1;32m    348\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:393\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[0;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m    392\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[0;32m--> 393\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:1093\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1092\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m-> 1093\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/callbacks.py:1169\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1165\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1168\u001B[0m     \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[0;32m-> 1169\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1170\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:694\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    691\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[0;32m--> 694\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py:631\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnest.map_structure\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap_structure\u001B[39m(func, \u001B[38;5;241m*\u001B[39mstructure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    547\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001B[39;00m\n\u001B[1;32m    548\u001B[0m \n\u001B[1;32m    549\u001B[0m \u001B[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001B[39;00m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnest_util\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnest_util\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mModality\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCORE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1066\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(modality, func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001B[39;00m\n\u001B[1;32m    970\u001B[0m \n\u001B[1;32m    971\u001B[0m \u001B[38;5;124;03m- For Modality.CORE: Refer to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001B[39;00m\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m modality \u001B[38;5;241m==\u001B[39m Modality\u001B[38;5;241m.\u001B[39mCORE:\n\u001B[0;32m-> 1066\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_tf_core_map_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m modality \u001B[38;5;241m==\u001B[39m Modality\u001B[38;5;241m.\u001B[39mDATA:\n\u001B[1;32m   1068\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _tf_data_map_structure(func, \u001B[38;5;241m*\u001B[39mstructure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001B[0m, in \u001B[0;36m_tf_core_map_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m   1101\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (_tf_core_flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m   1102\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _tf_core_pack_sequence_as(\n\u001B[1;32m   1105\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m-> 1106\u001B[0m     \u001B[43m[\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mentries\u001B[49m\u001B[43m]\u001B[49m,\n\u001B[1;32m   1107\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites,\n\u001B[1;32m   1108\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1101\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (_tf_core_flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m   1102\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _tf_core_pack_sequence_as(\n\u001B[1;32m   1105\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m-> 1106\u001B[0m     [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m   1107\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites,\n\u001B[1;32m   1108\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:687\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    684\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[1;32m    685\u001B[0m     \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[1;32m    686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 687\u001B[0m         t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001B[39;00m\n\u001B[1;32m    689\u001B[0m     \u001B[38;5;66;03m# as-is.\u001B[39;00m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:394\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \n\u001B[1;32m    373\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[0;32m--> 394\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:360\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    359\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    361\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "seq2seq_rnn.fit(train_ds, epochs=1, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T21:32:16.875788Z",
     "start_time": "2024-07-29T21:31:01.382111Z"
    }
   },
   "id": "365a2c675d37c41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you work on a real-world machine translation system, you will likely use “BLEU scores” to evaluate your models—a metric that looks at entire generated sequences and that seems to correlate well with human perception of translation quality.\n",
    "\n",
    "The RNN approach to sequence-to-sequence learning has a few fundamental limitations:\n",
    "- The source sequence representation has to be held entirely in the encoder state vector(s), which puts significant limitations on the size and complexity of the sentences you can translate. It’s a bit as if a human were translating a sentence entirely from memory, without looking twice at the source sentence while producing the translation.\n",
    "- RNNs have trouble dealing with very long sequences, since they tend to progressively forget about the past—by the time you’ve reached the 100th token in either sequence, little information remains about the start of the sequence. That means RNN-based models can’t hold onto long-term context, which can be essential for translating long documents.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1004f48b355d83f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I asked him to send us the book.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 20), found shape=(None, 21)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(input_sentence)\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdecode_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_sentence\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[24], line 14\u001B[0m, in \u001B[0;36mdecode_sequence\u001B[0;34m(input_sentence)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_decoded_sentence_length):\n\u001B[1;32m     13\u001B[0m     tokenized_target_sentence \u001B[38;5;241m=\u001B[39m target_vectorization([decoded_sentence])\n\u001B[0;32m---> 14\u001B[0m     next_token_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mseq2seq_rnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mtokenized_input_sentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenized_target_sentence\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     sampled_token_index \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(next_token_predictions[\u001B[38;5;241m0\u001B[39m, i, :])\n\u001B[1;32m     17\u001B[0m     sampled_token \u001B[38;5;241m=\u001B[39m spa_index_lookup[sampled_token_index]\n",
      "File \u001B[0;32m~/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/by/tj028j115v58k1r17jkmbr5c0000gq/T/__autograph_generated_filevkmsjiam.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/osamaabuhamdan/PycharmProjects/DNNResearchProject/venv/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 20), found shape=(None, 21)\n"
     ]
    }
   ],
   "source": [
    "# Using our code in inference\n",
    "import numpy as np\n",
    "\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T21:16:01.380358Z",
     "start_time": "2024-07-29T21:16:00.211475Z"
    }
   },
   "id": "e03960248983baf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequence-to-sequence learning with Transformer\n",
    "Sequence-to-sequence learning is the task where Transformer really shines. Neural attention enables Transformer models to successfully process sequences that are considerably longer and more complex than those RNNs can handle.\n",
    "\n",
    "As a human translating English to Spanish, you’re not going to read the English sentence one word at a time, keep its meaning in memory, and then generate the Spanish sentence one word at a time. That may work for a five-word sentence, but it’s unlikely to work for an entire paragraph. Instead, you’ll probably want to go back and forth between the source sentence and your translation in progress, and **pay attention** to different words in the source as you’re writing down different parts of your translation.\n",
    "\n",
    "In a sequence-to-sequence Transformer, the Transformer encoder would naturally play the role of the encoder, which reads the source sequence and produces an encoded representation of it. Unlike our previous RNN encoder (after processing the entire input sequence, the output is a context vector), the Transformer encoder keeps the encoded representation in a sequence format: it's a sequence of context-aware embedding vectors.\n",
    "\n",
    "The second half of the model is the **Transformer decoder**. Just like the RNN decoder, it reads tokens 0...N in the target sequence and tries to predict token N+1 with neural attention.\n",
    " \n",
    "In a Transformer decoder:\n",
    "- **The target sequence serves as an attention \"query\"**.\n",
    "- **The source sequence plays the roles of both keys and values.** \n",
    "i.e. The queries come from the decoder, while the keys and values come from the encoder's output.\n",
    "<img src=\"static/seq2seq_with_transformer.png\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c3c2628810bf5b2"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        # This attribute ensures that the layer will propagate its input mask to its outputs; masking in Keras is explicitly opt-in.\n",
    "        # If you pass a mask to a layer that doesn't implement compute_mask() and that doesn't expose this supports_masking attribute, that’s an error.\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:15:57.344758Z",
     "start_time": "2024-07-30T00:15:57.269466Z"
    }
   },
   "id": "3d662b5f1095f3b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Causal Padding:\n",
    "Causal padding is a technique used in sequence-to-sequence models, particularly in Transformer decoders, to ensure that the model does not use information from future tokens when predicting the next token in a sequence. \n",
    "\n",
    "### Why Causal Padding is Needed:\n",
    "In Transformer models, the self-attention mechanism allows each token in a sequence to attend to all other tokens, including future tokens. While this is beneficial for understanding the context in many applications, it poses a problem in autoregressive sequence generation tasks like language modeling and translation, where future tokens should not be known during prediction.\n",
    "\n",
    "### How Causal Padding Works\n",
    "Causal padding is implemented by masking out future tokens in the attention mechanism. This means that when predicting the next token in a sequence, the model can only attend to the tokens that have already been generated and not the ones that come after."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d163be5f14833cb"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dime = 2048\n",
    "num_heads = 8\n",
    "sequence_length = 20\n",
    "vocab_size = 15000\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:25:10.627848Z",
     "start_time": "2024-07-30T00:25:10.023402Z"
    }
   },
   "id": "c4fb7ca6a5be79e9"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 118s 89ms/step - loss: 5.0949 - accuracy: 0.2865 - val_loss: 4.5936 - val_accuracy: 0.3098\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x3ef8a9910>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=1, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T00:27:09.430534Z",
     "start_time": "2024-07-30T00:25:10.621325Z"
    }
   },
   "id": "1367db035003f5a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can use the previous code for inference "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99102a311fb8652"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mother of Summaries\n",
    "- There are two kinds of NLP models: bag-of-words models that process sets of words or N-grams without taking into account their order, and sequence models that process word order. A bag-of-words model is made of Dense layers, while a sequence model could be an RNN, a 1D convnet, or a Transformer.\n",
    "- When it comes to text classification, the ratio between the number of samples in your training data and the mean number of words per sample can help you determine whether you should use a bag-of-words model or a sequence model.\n",
    "- Word embeddings are vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words.\n",
    "- Sequence-to-sequence learning is a generic, powerful learning framework that can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence model is made of an encoder, which processes a source sequence, and a decoder, which tries to predict future tokens in target sequence by looking at past tokens, with the help of the encoder-processed source sequence.\n",
    "- Neural attention is a way to create context-aware word representations. It’s the basis for the Transformer architecture.\n",
    "- The Transformer architecture, which consists of a TransformerEncoder and a TransformerDecoder, yields excellent results on sequence-to-sequence tasks. The first half, the TransformerEncoder, can also be used for text classification or any sort of single-input NLP task.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0797c8dac9b5f82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "68d460304801f6a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
