{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4.4 Regularizing your model\n",
    "Regularization techniques are a set of best practices that actively impede the model’s ability to fit perfectly to the training data, with the goal of making the model perform better during validation. This is called “regularizing” the model, because it tends to make the model simpler, more “regular,” its curve smoother, more “generic”; thus it is less specific to the training set and better able to generalize by more closely approximating the latent manifold of the data.\n",
    "\n",
    "Let’s review some of the most common regularization techniques and apply them in practice to improve the movie-classification model from chapter 4."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5217d067a723cf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1- Reducing the network’s size\n",
    "The simplest way to mitigate overfitting is to reduce the size of the model by reducing The number of learnable parameters in the model by:\n",
    "1. Reducing The number of layers\n",
    "2. Reducing The number of units per layer\n",
    "\n",
    "You have to find a balance for your model, so it has:\n",
    "1. Limited memorization resources, so it won’t be able to simply memorize its training data.\n",
    "2. Enough parameters that they don’t underfit."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a19ad00c1efb66c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- Adding weight Regularization\n",
    "- It’s to put constraints on the complexity of a model by forcing its weights to take only small values, which makes the distribution of weight values more regular.\n",
    "- It’s done by adding to the loss function of the model a cost associated with having large weights. This cost comes in two flavors:\n",
    "    1. L1 regularization—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).\n",
    "    2. L2 regularization (weight decay)—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1d604ace90af06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af994b9143e5768"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
